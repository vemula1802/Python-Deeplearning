import matplotlib.pylab as plt
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, Input
from keras.models import Sequential
from keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from pathlib import Path
from keras.optimizers import Adam,RMSprop,SGD
import pandas as pd
import random
import os
import cv2
from keras.preprocessing.image import img_to_array, load_img
import numpy as np
from keras.utils import np_utils
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.constraints import maxnorm


df = pd.read_csv("/content/drive/My Drive/monkey_labels.txt")
print(df)
#label info
cols = ['Label','Latin Name', 'Common Name','Train Images', 'Validation Images']
labels = pd.read_csv("/content/drive/My Drive/monkey_labels.txt", names=cols, skiprows=1)
labels = labels['Common Name']
print(labels)

height=150
width=150
channels=3
batch_size=32
seed=1337

train_dir = Path('/content/drive/My Drive/training/training/')
test_dir = Path('/content/drive/My Drive/validation/validation/')

# Training generator
train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=(height,width),batch_size=batch_size,seed=seed,class_mode='categorical')

# Test generator
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(test_dir, target_size=(height,width),batch_size=batch_size, seed=seed,class_mode='categorical')

def image_show(num_image,label): 
    for i in range(num_image):
        imgdir = Path('/content/drive/My Drive/training/training/'+ label)
        print(imgdir)
        imgfile = random.choice(os.listdir(imgdir))
        print(imgfile)
        img = cv2.imread('/content/drive/My Drive/training/training/'+ label +'/'+ imgfile)
        print(img.shape)
        print(label)
        plt.figure(i)
        plt.imshow(img)
        plt.title(imgfile)
    plt.show()

print(labels[4])
image_show(2,'n4')

def read_data(path):
    '''This function reads images from folder'''
    images = []
    labels = []
    count = -1
    for root, folder, file in os.walk(path):
        for f in file:
            file_path = os.path.join(root, f)
            img = load_img(file_path, target_size=(32, 32))
            img = img_to_array(img)
            img = img.reshape(img.shape)
            images.append(img)
            labels.append([count])
        count += 1
    images = np.array(images)
    labels = np.array(labels)
    return images, labels

#Read images from folders
x_train, y_train = read_data(train_dir)
x_test, y_test = read_data(test_dir)

#Normalize data
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Scaling
x_train = x_train / 255.0
x_test = x_test / 255.0

#One hot encode data
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]

# Creating model
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:], padding='same', activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dropout(0.2))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Compile model
epochs = 25
lrate = 0.01
decay = lrate/epochs
sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Fitting model
history=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=32)
print(model.summary())

print(history.history.keys())
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'red', label='Training acc')
plt.plot(epochs, val_acc, 'blue', label='Validation acc')
plt.legend()
plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'red', label='Training loss')
plt.plot(epochs, val_loss, 'blue', label='Validation loss')

plt.legend()
plt.show()

# Save model
model.save('model_Q3.h5')

#Load model 
model = load_model('model_Q3.h5')
print(model.summary())

#Evaluate model
score = model.evaluate(x_test, y_test, verbose=0)
print('Loss:', score[0])
print('Accuracy: ', score[1])

# Prediction on model
num_to_text = {
  0: "mantled_howler",               
  1: "patas_monkey",                  
  2: "bald_uakari",                   
  3: "japanese_macaque",              
  4: "pygmy_marmoset",                
  5: "white_headed_capuchin",         
  6: "silvery_marmoset",              
  7: "common_squirrel_monkey",        
  8: "black_headed_night_monkey",     
  9: "nilgiri_langur"      
}
test_images = x_test
test_labels = y_test

def make_prediction(i):
    test_img = test_images[i]
    test_data= x_test[[i], :]

    plt.imshow(test_img, cmap=plt.get_cmap('gray'))
    plt.title("Model Prediction: {}".format(num_to_text[model.predict_classes(test_data)[0]]))
    plt.show()

prediction_idx = [0,1, 2, 3]

for idx in prediction_idx:
    make_prediction(idx)



# -*- coding: utf-8 -*-
"""task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qt9DI1wCsJeSNHXlh1fCH0n_p07ZKpg-
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from keras.preprocessing import sequence
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from sklearn.datasets import fetch_20newsgroups
from keras.models import Sequential

data = fetch_20newsgroups(subset='train')
data.target_names

max_features = 20000
batch_size = 32
k=['alt.atheism','rec.autos']
train_data = fetch_20newsgroups(subset='train', categories=k, random_state=32)
test_data = fetch_20newsgroups(subset='test',categories=k, random_state=32)

train_target = train_data.target

test_target = test_data.target

vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.95)
train_data = vectorizer.fit_transform(train_data.data)
test_data = vectorizer.transform(test_data.data)
train_data = train_data.todense()
test_data = test_data.todense()

train_data.shape

test_data.shape

train_target.shape

test_target.shape

n_train = train_data.shape[0]
train_data = train_data[:n_train, :]
train_target = train_target[:n_train]

model = Sequential()
model.add(Embedding(max_features, 256))
model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))
model.add(Dense(1, activation='softmax'))
model.compile(loss='binary_crossentropy',
             optimizer='rmsprop',
              metrics=['accuracy'])
# model.fit(train_data, train_labels,
#           batch_size=batch_size,
#           epochs=1,
# validation_data=(test_data, train_labels))
history = model.fit(train_data, train_target, epochs=2, verbose=True, validation_data=(train_data, train_target), batch_size=32)
score, accuracy = model.evaluate(test_data, test_target, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', accuracy)
# print(train_data.shape)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train_data', 'test_data'], loc='upper left')
plt.show()

# For Loss Values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train_data', 'test_data'], loc='upper left')
plt.show()

from keras.models import Sequential
from keras import layers
from keras.preprocessing.text import Tokenizer
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_20newsgroups
from keras.layers.embeddings import Embedding
from keras.layers import Flatten


twenty_train = fetch_20newsgroups(subset='train', shuffle=True)
y = twenty_train.target
sentences = twenty_train.data

max_review_len = max([len(s.split()) for s in sentences])


tokenizer = Tokenizer(num_words=max_review_len)
tokenizer.fit_on_texts(sentences)

sentences = tokenizer.texts_to_matrix(sentences)

le = preprocessing.LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)

# Build model
model = Sequential()
model.add(layers.Dense(300, input_dim=max_review_len, activation='relu'))
model.add(layers.Dense(20,activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=5, verbose=True, validation_data=(X_test, y_test), batch_size=256)

# Output
loss, accuracy = model.evaluate(X_test, y_test)
print("Loss: {}".format(loss))
print("Accuracy: {}".format(accuracy))

# For Accuracy Values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# For Loss Values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()